---
title: "House Prices Prediction"
datePublished: Fri Nov 08 2024 21:00:09 GMT+0000 (Coordinated Universal Time)
cuid: cm3980f5l000009jx88l62ocn
slug: house-prices-prediction
cover: https://cdn.hashnode.com/res/hashnode/image/upload/v1731097221521/9a3c06b4-d6b0-4084-bf1d-fc421f53b8de.jpeg
ogImage: https://cdn.hashnode.com/res/hashnode/image/upload/v1731099468314/c75cfce7-e351-4ffc-aeab-42a4e1eed247.jpeg
tags: python, machine-learning, prediction, linearregression, knn-algorithms

---

Dans le cadre de l'analyse pr√©dictive en immobilier, j'ai entrepris un projet visant √† estimer les prix des maisons en utilisant des techniques de machine learning. Ce projet s'appuie sur le dataset "USA Housing", qui fournit diverses informations sur les propri√©t√©s r√©sidentielles aux √âtats-Unis.

#### R√©f√©rences du projet

Dataset : [Data](https://github.com/miirshe/USA-Housing-Analysis-and-Prediction-Price/blob/main/USA_Housing.csv)

Notebook : [Predicting House Prices](https://aurvl.github.io/NotebookSites/housing/housing.html)

Directory : [Housing](https://github.com/aurvl/ML_projects/blob/main/Housing/Housing.md)

## **Objectif** du Projet

L'objectif principal est de d√©velopper un mod√®le capable de pr√©dire avec pr√©cision le prix de vente d'une maison en fonction de plusieurs caract√©ristiques, telles que le revenu moyen de la zone, l'√¢ge moyen des maisons, le nombre moyen de pi√®ces et de chambres, ainsi que la population de la zone.

## Exploration des Donn√©es

```python
df.head()
```

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1731097054792/bcfd77ee-242a-4b37-aa2d-6aedcadadeea.png align="center")

Le dataset comprend 5 000 enregistrements et 7 colonnes :

* **Avg. Area Income** : Revenu moyen des r√©sidents de la zone.
    
* **Avg. Area House Age** : √Çge moyen des maisons dans la zone.
    
* **Avg. Area Number of Rooms** : Nombre moyen de pi√®ces par maison.
    
* **Avg. Area Number of Bedrooms** : Nombre moyen de chambres par maison.
    
* **Area Population** : Population de la zone.
    
* **Price** : Prix de vente de la maison.
    
* **Address** : Adresse de la maison.
    

Une analyse initiale a r√©v√©l√© que les colonnes "Avg. Area Income" et "Avg. Area House Age" contiennent chacune 10 valeurs manquantes. Les autres colonnes sont compl√®tes.

```python
df.info()
```

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1731097101578/a36d4ea5-ddcc-48e2-8649-fe3756562e6c.png align="center")

## Analyse Statistique Descriptive

Les statistiques descriptives montrent que le prix moyen des maisons est d'environ 1 232 073 $, avec une variation significative allant de 15 939 $ √† 2 469 066 $. Les maisons ont en moyenne 7 pi√®ces et pr√®s de 4 chambres, avec un √¢ge moyen d'environ 6 ans. Le revenu moyen des zones est de 68 564 $, et la population moyenne est d'environ 36 914 habitants.

## Visualisation des donn√©es

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1731097512497/dfffcaac-19f8-4877-b03e-186e028f8fea.png align="center")

Ce pairplot r√©v√®le la pr√©sence de valeurs aberrantes dans certaines variables (nombre de pi√®ces et population), avec des valeurs largement au-del√† de la norme, ce qui pourrait biaiser l‚Äôanalyse et n√©cessite un examen attentif ou un ajustement.

Ensuite, en analysant la relation entre le prix et les autres caract√©ristiques des logements, on constate que :

* **Prix vs. Revenu Moyen de la Zone** : Il existe une relation positive entre le prix et le revenu moyen de la zone, ce qui sugg√®re que les zones avec des revenus plus √©lev√©s ont des prix de logements plus √©lev√©s.
    
* **Prix vs. Nombre Moyen de Pi√®ces** : On observe une l√©g√®re tendance positive : les maisons avec plus de pi√®ces tendent √† √™tre plus ch√®res.
    
* **Prix vs. Population de la Zone** : On observe une relation positive entre le prix et la population de la zone, indiquant que la densit√© de population est pas un facteur d√©terminant pour les prix, d'autres √©l√©ments comme le revenu et les commodit√©s jouant probablement un r√¥le plus important.
    

## Pr√©paration des Donn√©es

Avant de construire le mod√®le, il est essentiel de traiter les valeurs manquantes et de normaliser les donn√©es pour assurer une performance optimale du mod√®le. Des techniques telles que l'**imputation des donn√©es manquantes** et la **normalisation des variables** num√©riques ont √©t√© appliqu√©es √† partir d‚Äôune pipeline ci-dessous.

```python
import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
import pickle

def preprocess(df, train):
    """
    This function handles missing and aberrant (outlier) values in a DataFrame by replacing them with the median of the respective column's distribution.
    It also applies scaling to standardize the data.

    Args:
        df (DataFrame): The dataset containing columns with missing or aberrant values.
        train (bool): A flag indicating whether the function is being applied to a training set (True) or a test set (False).
                      If True, the function will fit and save an imputer, scaler, and outlier bounds; if False, it will load and apply the saved models and bounds.

    Returns:
        DataFrame: A transformed DataFrame with missing and aberrant values replaced by the median, and features standardized.
    
    Process:
        1. Outlier Detection and Replacement: Outliers are identified using the IQR (Interquartile Range) method.
           Values below Q1 - 1.5*IQR or above Q3 + 1.5*IQR are considered outliers and replaced with NaN.
        2. Imputation: Missing values (including outliers replaced by NaN) are filled with the median of each column.
           If train is True, an imputer is fit and saved for later use. If train is False, the previously saved imputer is loaded.
        3. Scaling: Standardizes the data to have a mean of 0 and a standard deviation of 1.
           If train is True, a scaler is fit and saved; otherwise, the saved scaler is loaded and applied.
    """
    
    # Step 1: Handle outliers based on the IQR method
    if train:
        # Calculate and save outlier bounds for each column in the training data
        outlier_bounds = {}
        for col in df.columns:
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            outlier_bounds[col] = (lower_bound, upper_bound)
            
            # Replace outliers with NaN in training data
            df.loc[(df[col] < lower_bound) | (df[col] > upper_bound), col] = np.nan
        
        # Save the outlier bounds
        with open('outlier_bounds.pickle', 'wb') as f:
            pickle.dump(outlier_bounds, f)
    else:
        # Load and apply outlier bounds in the test data
        with open('outlier_bounds.pickle', 'rb') as f:
            outlier_bounds = pickle.load(f)
        
        for col in df.columns:
            lower_bound, upper_bound = outlier_bounds[col]
            df.loc[(df[col] < lower_bound) | (df[col] > upper_bound), col] = np.nan
    
    # Step 2: Impute missing values with the median
    if train:
        imputer = SimpleImputer(strategy='median')
        imputer.fit(df)
        with open('imputer.pickle', 'wb') as f:
            pickle.dump(imputer, f)
    else:
        with open('imputer.pickle', 'rb') as f:
            imputer = pickle.load(f)
    
    df = pd.DataFrame(imputer.transform(df), columns=df.columns)
    
    # Step 3: Standardize the data
    if train:
        scaler = StandardScaler()
        scaler.fit(df)
        with open('scaler.pickle', 'wb') as f:
            pickle.dump(scaler, f)
    else:
        with open('scaler.pickle', 'rb') as f:
            scaler = pickle.load(f)
    
    df = pd.DataFrame(scaler.transform(df), columns=df.columns)

    return df
```

## Mod√©lisation

Plusieurs algorithmes de machine learning ont √©t√© explor√©s pour pr√©dire les prix des maisons, notamment :

* **R√©gression Lin√©aire** : Mod√®le simple qui tente de trouver la relation lin√©aire entre les variables ind√©pendantes et le prix.
    
* **K-Nearest Neighbors (KNN)** : Algorithme qui pr√©dit le prix d'une maison en se basant sur les prix des maisons les plus proches dans l‚Äôespace des variables. Il utilise la distance entre les points pour identifier les "voisins" et estime le prix en moyenne des valeurs proches.
    

Chaque mod√®le a √©t√© √©valu√© en utilisant des m√©triques telles que la Mean Squared Error (MSE) et le coefficient de d√©termination (R¬≤) pour d√©terminer sa performance pr√©dictive.

## R√©sultats

Les performances des mod√®les sont les suivantes :

* **R√©gression Lin√©aire** : MSE moyen de **15350399432.8** avec un R¬≤ de **86%**.
    
* **KNN** : MSE moyen de **22565570117.23** avec un √©cart-type de **75%**.
    

Ces r√©sultats montrent que la R√©gression Lin√©aire offre la meilleure performance pr√©dictive parmi les mod√®les test√©s. Cependant, son MSE √©lev√© r√©v√®le des limitations importantes. Ce qui signifie qu'il est possible d'am√©liorer encore la pr√©cision du mod√®le.

## Conclusion

Dans cette √©tude, deux mod√®les pr√©dictifs, la r√©gression lin√©aire et K-Nearest Neighbors (KNN), ont √©t√© test√©s pour estimer les prix des maisons. La r√©gression lin√©aire a obtenu un R¬≤ d'environ 86%, indiquant une bonne explication de la variance des prix, bien que le MSE reste √©lev√©, signalant des limites. KNN a pr√©sent√© un R¬≤ plus faible de 75%, montrant une moins bonne ad√©quation aux donn√©es.

La r√©gression lin√©aire s'est av√©r√©e plus efficace, capturant mieux la variance des prix avec une meilleure pr√©cision et interpr√©tabilit√©.

Des mod√®les plus complexes ou un travail de feature engineering pourraient √™tre envisag√©s pour am√©liorer les r√©sultats.

Pour une exploration plus d√©taill√©e et le code source complet, vous pouvez consulter le repository du projet sur Github ([HOUSING](https://github.com/aurvl/ML_projects/blob/main/Housing/Housing.md)).

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1731099360857/56df0946-3aed-4593-8a77-77ef75d3c871.png align="center")

*Merci et enjoy !üéâ*